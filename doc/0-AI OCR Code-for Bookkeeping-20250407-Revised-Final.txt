# Enhanced AI OCR System with Table detection and structuring, GPT-4 Vision fallback, Excel export with multiple sheets, Visual field annotation, JSON and DB output, Multithreaded batch support

import os
import re
import json
import cv2
import base64
import numpy as np
import pytesseract
import spacy
import logging
import pandas as pd
from pathlib import Path
from pdf2image import convert_from_path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import sqlite3
import layoutparser as lp
import openai
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.styles import Font, PatternFill

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load NLP model
try:
    nlp = spacy.load("en_core_web_sm")
except Exception as e:
    logger.warning("SpaCy model not found. Please run: python -m spacy download en_core_web_sm")
    nlp = None

# Set OpenAI API Key (assumes env var OPENAI_API_KEY is set)
openai.api_key = os.getenv("OPENAI_API_KEY")

class AI_OCR:
    def __init__(self, tesseract_path: Optional[str] = None, output_dir: str = "output"):
        self.output_dir = output_dir
        Path(output_dir).mkdir(exist_ok=True)
        if tesseract_path:
            pytesseract.pytesseract.tesseract_cmd = tesseract_path
        self._check_tesseract()
        self.model = lp.Detectron2LayoutModel("lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config")

    def _check_tesseract(self):
        try:
            version = pytesseract.get_tesseract_version()
            logger.info(f"Tesseract version: {version}")
        except:
            logger.error("Tesseract not found or misconfigured.")

    def _render_table_from_image(self, image: np.ndarray) -> pd.DataFrame:
        data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
        n_boxes = len(data['level'])
        rows = {}
        for i in range(n_boxes):
            if int(data['conf'][i]) < 0 or not data['text'][i].strip():
                continue
            y = data['top'][i]
            text = data['text'][i].strip()
            row_key = y // 10
            rows.setdefault(row_key, []).append((data['left'][i], text))
        structured_rows = []
        for y_key in sorted(rows):
            row = sorted(rows[y_key], key=lambda x: x[0])
            structured_rows.append([cell for _, cell in row])
        return pd.DataFrame(structured_rows)

    def _annotate_fields(self, image: np.ndarray, result: Dict[str, Any], filename: str):
        annotated = image.copy()
        fields = result.get("fields", {})
        colors = {'vendor': (0, 255, 0), 'amounts': (255, 0, 0), 'dates': (0, 0, 255)}
        for field, color in colors.items():
            if field == "vendor" and fields.get("vendor"):
                text = fields["vendor"]
            elif field == "amounts" and fields.get("amounts"):
                text = str(max(fields["amounts"]))
            elif field == "dates" and fields.get("dates"):
                text = fields["dates"][0]
            else:
                continue
            boxes = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
            for i in range(len(boxes['text'])):
                if text.lower() in boxes['text'][i].lower():
                    (x, y, w, h) = (boxes['left'][i], boxes['top'][i], boxes['width'][i], boxes['height'][i])
                    cv2.rectangle(annotated, (x, y), (x + w, y + h), color, 2)
                    cv2.putText(annotated, field, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                    break
        out_path = os.path.join(self.output_dir, f"{Path(filename).stem}_annotated.png")
        cv2.imwrite(out_path, annotated)
        logger.info(f"Annotated image saved: {out_path}")

    def _export_to_excel(self, result: Dict[str, Any], original_path: str):
        items = result.get("fields", {}).get("line_items", [])
        excel_path = os.path.join(self.output_dir, f"{Path(original_path).stem}_line_items.xlsx")
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            if items:
                pd.DataFrame(items).to_excel(writer, index=False, sheet_name="LineItems")
            for i, table in enumerate(result.get("tables", [])):
                if "structured" in table:
                    df = pd.DataFrame(table["structured"])
                    df.to_excel(writer, index=False, sheet_name=f"Table_Page{table['page']}_{i+1}")
        logger.info(f"Exported structured tables to Excel: {excel_path}")

    def _detect_tables(self, image: np.ndarray, source: str, index: int) -> List[Dict[str, Any]]:
        layout = self.model.detect(image)
        table_blocks = layout.filter_by("Table")
        tables = []
        for i, block in enumerate(table_blocks):
            x1, y1, x2, y2 = map(int, block.coordinates)
            roi = image[y1:y2, x1:x2]
            structured_df = self._render_table_from_image(roi)
            table_path = os.path.join(self.output_dir, f"{Path(source).stem}_page{index+1}_table{i+1}.png")
            cv2.imwrite(table_path, roi)
            tables.append({"structured": structured_df.values.tolist(), "coordinates": [x1, y1, x2, y2], "image_file": table_path, "page": index + 1})
        return tables

    def _fallback_to_openai(self, image: np.ndarray) -> Dict[str, Any]:
        _, buffer = cv2.imencode('.png', image)
        img_b64 = base64.b64encode(buffer).decode('utf-8')
        response = openai.ChatCompletion.create(
            model="gpt-4-vision-preview",
            messages=[{"role": "user", "content": [
                {"type": "text", "text": "Extract the vendor, date, total, tax, payment method, and summarize what this document is about."},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}"}}
            ]}],
            max_tokens=1000
        )
        text = response.choices[0].message.content
        return {"summary": text}

    def _extract_fields(self, text: str) -> Dict[str, Any]:
        return {
            "vendor": self._extract_vendor(text),
            "dates": self._extract_dates(text),
            "amounts": self._extract_amounts(text),
            "line_items": self._extract_line_items(text)
        }

    def _extract_vendor(self, text: str) -> str:
        if not nlp:
            return text.split('\n')[0].strip()
        doc = nlp(text[:1000])
        orgs = [ent.text for ent in doc.ents if ent.label_ == "ORG"]
        return orgs[0] if orgs else ""

    def _extract_dates(self, text: str) -> List[str]:
        patterns = [r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b', r'\b\d{4}[/-]\d{1,2}[/-]\d{1,2}\b']
        dates = []
        for pattern in patterns:
            dates.extend(re.findall(pattern, text))
        return dates

    def _extract_amounts(self, text: str) -> List[float]:
        matches = re.findall(r'\$\s*\d+[,.]?\d*', text)
        return [float(m.replace("$", "").replace(",", "")) for m in matches if m]

    def _extract_line_items(self, text: str) -> List[Dict[str, Any]]:
        lines = text.split('\n')
        items = []
        for line in lines:
            if re.search(r'\$\s*\d+\.\d{2}', line):
                items.append({"description": line.strip()})
        return items

    def _save_result(self, result: Dict[str, Any], filename: str):
        out_path = os.path.join(self.output_dir, f"{os.path.splitext(filename)[0]}_result.json")
        with open(out_path, 'w') as f:
            json.dump(result, f, indent=2)
        logger.info(f"Saved result to {out_path}")

    def _save_to_database(self, result: Dict[str, Any]):
        conn = sqlite3.connect("ocr_results.db")
        c = conn.cursor()
        c.execute('''CREATE TABLE IF NOT EXISTS documents
                     (vendor TEXT, dates TEXT, amounts TEXT, file_path TEXT)''')
        vendor = result.get("fields", {}).get("vendor", "")
        dates = ",".join(result.get("fields", {}).get("dates", []))
        amounts = ",".join([str(a) for a in result.get("fields", {}).get("amounts", [])])
        file_path = result.get("file_path", "")
        c.execute("INSERT INTO documents VALUES (?, ?, ?, ?)", (vendor, dates, amounts, file_path))
        conn.commit()
        conn.close()

    def _process_image_data(self, image: np.ndarray, source: str, index: int = 0) -> Dict[str, Any]:
        processed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        ocr_output = pytesseract.image_to_string(processed)
        if len(ocr_output.strip()) < 50:
            fallback = self._fallback_to_openai(image)
            return {"text": fallback.get("summary", ""), "fields": fallback, "ocr_data": {}, "tables": []}
        fields = self._extract_fields(ocr_output)
        tables = self._detect_tables(image, source, index)
        self._annotate_fields(image, {"fields": fields}, source)
        return {"text": ocr_output, "fields": fields, "ocr_data": {}, "tables": tables}

    def process_document(self, file_path: str) -> Dict[str, Any]:
        ext = os.path.splitext(file_path)[1].lower()
        if ext == ".pdf":
            images = convert_from_path(file_path)
            results = [self._process_image_data(np.array(img), file_path, i) for i, img in enumerate(images)]
            combined = self._combine_results(results)
        else:
            image = cv2.imread(file_path)
            combined = self._process_image_data(image, file_path)
        combined["file_path"] = file_path
        self._save_result(combined, os.path.basename(file_path))
        self._export_to_excel(combined, file_path)
        self._save_to_database(combined)
        return combined

    def _combine_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        text = "\n".join([r["text"] for r in results])
        all_items = sum([r["fields"].get("line_items", []) for r in results if "fields" in r], [])
        all_tables = sum([r.get("tables", []) for r in results], [])
        return {
            "text": text,
            "fields": {
                "vendor": results[0]["fields"].get("vendor", "") if "fields" in results[0] else "",
                "dates": sum([r["fields"].get("dates", []) for r in results if "fields" in r], []),
                "amounts": sum([r["fields"].get("amounts", []) for r in results if "fields" in r], []),
                "line_items": all_items
            },
            "tables": all_tables
        }

    def process_batch(self, input_dir: str):
        files = [os.path.join(input_dir, f) for f in os.listdir(input_dir)
                 if f.lower().endswith((".pdf", ".jpg", ".png", ".jpeg", ".tiff"))]
        with ThreadPoolExecutor(max_workers=4) as executor:
            executor.map(self.process_document, files)

if __name__ == "__main__":
    processor = AI_OCR()
    processor.process_batch("sample_documents")
